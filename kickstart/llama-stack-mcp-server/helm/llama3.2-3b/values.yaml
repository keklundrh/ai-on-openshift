# Default values for llama3.2-3b
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  name: ""

# Model configuration
model:
  maxModelLen: 8192


# Resource requirements (adjust based on GPU availability)
resources:
  limits:
    nvidia.com/gpu: 1
    memory: 24Gi
    cpu: 4
  requests:
    nvidia.com/gpu: 1
    memory: 16Gi
    cpu: 2


nodeSelector:
  nvidia.com/gpu.present: "true"

tolerations:
  - effect: NoSchedule
    key: nvidia.com/gpu
    value: NVIDIA-A10G-SHARED

affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: nvidia.com/gpu.present
          operator: In
          values:
          - "true"


# Environment variables
env:
  CUDA_VISIBLE_DEVICES: "0"
  TRANSFORMERS_CACHE: "/root/.cache/huggingface"
  HF_HOME: "/root/.cache/huggingface"

# InferenceService configuration
inferenceService:
  displayName: "llama3.2-3b"
  maxReplicas: 1
  minReplicas: 1
  modelFormat: "vLLM"
  modelName: ""
  storageUri: "oci://quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-3b-instruct"

# Serving Runtime configuration
servingRuntime:
  enabled: true  # Set to true to create a ServingRuntime resource
  image: "quay.io/modh/vllm@sha256:0d55419f3d168fd80868a36ac89815dded9e063937a8409b7edf3529771383f3"
  shmSizeLimit: "1Gi"
  memBufferBytes: 134217728  # 128MB
  modelLoadingTimeoutMillis: 90000  # 90 seconds